{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook # progress bars in Jupyter\n",
    "from time import time # measure the computation time of a python code\n",
    "import pandas as pd # the most basic & powerful data manipulation tool\n",
    "import numpy as np # Here, mostly used for np.nan\n",
    "import langdetect # detect the language of text\n",
    "import stop_words # handles stop words in many languages without having to rebuild them everytime\n",
    "import spacy # NLP library for POS tagging\n",
    "# For spacy use \"pip install spacy\", then \"python -m spacy download en\" to download English text mining modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "tqdm_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = pd.read_csv('https://raw.githubusercontent.com/SebastianS09/DataCampX/master/group10_applesupport.csv')\n",
    "df_comments.drop([\"Unnamed: 0\",\"date\",\"link\",\"nb_replies\",\"nb_views\",\"user\"],axis=1,inplace=True)\n",
    "df_comments.columns = [\"source\",\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_comments.source.value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean dataset\n",
    "\n",
    "1) Keep English (or the language of your choice) <br>\n",
    "2) Remove empty articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langdetect.detect raises errors that are not explained => better to use try/except combination\n",
    "def detect(text):\n",
    "    try:\n",
    "        return langdetect.detect(text)\n",
    "    except langdetect.detector.LangDetectException:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments['lang'] = df_comments.text.progress_map(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See comments with uncommon languages\n",
    "df_comments.loc[df_comments.lang=='af']\n",
    "# Looks like the language detection did not work well on these comments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#all the af are actually english\n",
    "df_comments.lang[df_comments.lang=='af']=\"en\"\n",
    "\n",
    "\n",
    "# Out of curiosity, check the languages of the articles\n",
    "df_comments.groupby('source').lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = df_comments[df_comments['lang'] == 'en' ].reset_index()\n",
    "df_english= df_english.replace(r'\\n',' ', regex=True)\n",
    "df_english = df_english.replace('\\xa0','', regex = True)\n",
    "df_english = df_english.replace('  ', ' ', regex = True)\n",
    "df_english.groupby('source').lang.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import models, corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stop_words.get_stop_words(language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the syntax of SpaCy (check documentation online for tutorials https://spacy.io/usage/)\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use SpaCy nlp() object from the line above, to analyse comments\n",
    "df_english['nlp_spacy'] = df_english.text.progress_map(lambda comment: nlp(comment.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len counts the number of words\n",
    "df_english['nlp_spacy_len'] = df_english['nlp_spacy'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only nouns in comments, to define better topics\n",
    "df_english['noun_tokens'] = df_english.nlp_spacy.progress_map(\n",
    "    lambda n: [w.lemma_ for w in n if w.pos_=='NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Count the number of nouns per comment\n",
    "df_english['noun_tokens_len'] = df_english['noun_tokens'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english[['noun_tokens_len']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model on comments that have at least 8 nouns (abitrary number to keep comments with enough topic content)\n",
    "df_try = df_english.loc[df_english.noun_tokens_len>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_try[['noun_tokens_len']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use bigram and trigram to catch combination of 2/3 words that have a specific meaning together\n",
    "bigram = Phrases(df_try.noun_tokens.tolist(), min_count=3)\n",
    "trigram = Phrases(bigram[df_try.noun_tokens.tolist()], min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_try.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(trigram[bigram[df_try.noun_tokens.tolist()]])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sw = ['iphone', 'phone', '+', 'gb', 'edge', 'one', 'galaxy', 'samsung', 'galaxy_s8', 'plus', \n",
    "#      's5', 'series', 'x', 'x.', '5s', '6s', '7', '8+', 'android', 'apple', 'ios', 'i√¢\\x80\\x99m', \n",
    "#      's4', 's5', 's6', 's7', 's8', 'bestbuy', 'best_buy', 'wife', 'son', 'daughter']\n",
    "#no custom stop words for now except iphone\n",
    "sw = [\"iphone\"]\n",
    "tokens = list(trigram[bigram[df_try.noun_tokens.tolist()]])\n",
    "tokens = [[t for t in tok if t.lower() not in STOPWORDS+sw] for tok in tokens]\n",
    "tokens =[[t for t in tok if t.lower() not in STOPWORDS+sw] for tok in tokens]\n",
    "\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "print(dictionary)\n",
    "# ignore words that appear in less than 5 documents or more than 10% documents\n",
    "dictionary.filter_extremes(no_below=3)\n",
    "print(dictionary)\n",
    "corpus = [dictionary.doc2bow(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import TfidfModel, LsiModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "from gensim import matutils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=4, eta = [0.0001] * len(dictionary.keys()))\n",
    "vis_data = gensimvis.prepare(topics, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics.show_topics(num_topics=num_topics, num_words=10, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each article, print the beginning of the article, the topic allocation for this document, \n",
    "# and the 5 main keywords of the first 2 topics of this document\n",
    "\n",
    "import math # for rounding\n",
    "topics_names = topics.show_topics(num_topics=num_topics, num_words=5, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,article in enumerate(df_try.text.tolist()):\n",
    "    topics_in_article = topics[corpus[i]]\n",
    "    topics_in_article = [(topic[0], math.ceil(topic[1]*100)/100) for topic in topics_in_article] # rounded, for better printing\n",
    "    key_words_topic = []\n",
    "    \n",
    "    for topic in range(min(len(topics_in_article), 2)):\n",
    "        id_topic = topics_in_article[topic][0] # id of the topic\n",
    "        key_words_topic.append(topics_names[id_topic][1])\n",
    "    \n",
    "    print(\"article #\"+str(i)+\": \"+article[:400].replace('\\n','') + '\\n' + str(topics_in_article) + '\\n' +\n",
    "          '\\n'.join([\"keywords topic: \"+kw for kw in key_words_topic]))\n",
    "    print()\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topics.alpha)\n",
    "print(topics.eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph of words to detect problems\n",
    "\n",
    "Graph of words use words that are neighbors in sentences. <br>For instance, in the sentence \"Graph of words use words that are neighbors in sentences\", the table below describes the neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| word_1 | word_2 |\n",
    "|-|-|\n",
    "| Graph | of |\n",
    "| of | words |\n",
    "| words | use |\n",
    "| use | words |\n",
    "| words | that |\n",
    "| that | are |\n",
    "| are | neighbors |\n",
    "| neighbors | in |\n",
    "| in | sentences |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll handle different things as well:\n",
    "- keeping only nouns\n",
    "- using words that are the 2nd neighbors (neighbor of neighbor)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx # to analyse graphs in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We aggregate data from ALL the comments (in our cleaned dataframe)\n",
    "# And take the words (tokens) that are nouns\n",
    "clean_text = df_try.noun_tokens.tolist()\n",
    "\n",
    "# The functions below will help us build the dataframe of words that are neighbors\n",
    "def clean_stop_words_in_dataframe(df, stop_words):\n",
    "    idx_1 = df.loc[df[df.columns[0]].isin(stop_words)].index\n",
    "    idx_2 = df.loc[df[df.columns[1]].isin(stop_words)].index\n",
    "    return df.loc[~(df.index.isin(idx_1.append(idx_2)))]\n",
    "\n",
    "def word_neighbors(dist):\n",
    "    return clean_stop_words_in_dataframe(\n",
    "        pd.concat([pd.DataFrame([clean_sentence[:-dist], clean_sentence[dist:]]).T for clean_sentence in clean_text]) \\\n",
    "        .rename(columns={0:'w0', 1:'w1'}).reset_index(drop=True), stop_words=STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_text[0] # nouns of the first comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This creates a huge table of all the words that are neighbors and 2nd-order neighbors\n",
    "# For neighbors we use weight = 2, for 2nd-order neighbors we use weight = 1\n",
    "data_graph_of_words = word_neighbors(1).assign(weight=2).append(word_neighbors(2).assign(weight=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_graph_of_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sum the weights for all combinations of neighbors\n",
    "data_graph_of_words = data_graph_of_words.groupby(['w0', 'w1']).weight.sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.__version__\n",
    "# If you have previous versions, the function might be nx.from_pandas_dataframe()\n",
    "graph_of_words = nx.from_pandas_edgelist(data_graph_of_words, source='w0', target='w1', edge_attr='weight',create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We select the words that are neighbors (and 2nd-order neighbors) of the word \"problem\"\n",
    "graph_of_words_center = nx.ego_graph(graph_of_words, n='problem', radius=1)\n",
    "print(graph_of_words_center.size())\n",
    "print(len(graph_of_words_center))\n",
    "graph_of_words_center.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which words are the most connected to \"problem\"?\n",
    "# Degree is the weight\n",
    "pd.DataFrame.from_dict([dict(graph_of_words_center.degree(graph_of_words_center.nodes(), weight='weight'))]).T.rename(columns={0:'degree'}).reset_index().rename(columns={'index':'word'}).sort_values('degree', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the graph as it is\n",
    "nx.draw(graph_of_words_center, node_size=20)\n",
    "# It doesn't give us a lot of information, except that many words connected to \"problem\" are connected together\n",
    "# (there's more than one line for each red dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use PageRank algorithm to see if some words are more connected to others\n",
    "pagerank = pd.DataFrame.from_dict([nx.pagerank(G=graph_of_words, alpha=0.99)]).T.rename(columns={0:'pagerank'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It confirms what we had with LDA: \"phone\", \"screen\", \"iphone\"... are connected to too many words\n",
    "pagerank.sort_values('pagerank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's group words into communities, and see if it makes sense in terms of topics\n",
    "# The code is taken from the link below\n",
    "# https://stackoverflow.com/questions/43541376/how-to-draw-communities-with-networkx\n",
    "def community_layout(g, partition):\n",
    "    \"\"\"\n",
    "    Compute the layout for a modular graph.\n",
    "\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    g -- networkx.Graph or networkx.DiGraph instance\n",
    "        graph to plot\n",
    "\n",
    "    partition -- dict mapping int node -> int community\n",
    "        graph partitions\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pos -- dict mapping int node -> (float x, float y)\n",
    "        node positions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pos_communities = _position_communities(g, partition, scale=3.)\n",
    "\n",
    "    pos_nodes = _position_nodes(g, partition, scale=1.)\n",
    "\n",
    "    # combine positions\n",
    "    pos = dict()\n",
    "    for node in g.nodes():\n",
    "        pos[node] = pos_communities[node] + pos_nodes[node]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _position_communities(g, partition, **kwargs):\n",
    "\n",
    "    # create a weighted graph, in which each node corresponds to a community,\n",
    "    # and each edge weight to the number of edges between communities\n",
    "    between_community_edges = _find_between_community_edges(g, partition)\n",
    "\n",
    "    communities = set(partition.values())\n",
    "    hypergraph = nx.DiGraph()\n",
    "    hypergraph.add_nodes_from(communities)\n",
    "    for (ci, cj), edges in between_community_edges.items():\n",
    "        hypergraph.add_edge(ci, cj, weight=len(edges))\n",
    "\n",
    "    # find layout for communities\n",
    "    pos_communities = nx.spring_layout(hypergraph, **kwargs)\n",
    "\n",
    "    # set node positions to position of community\n",
    "    pos = dict()\n",
    "    for node, community in partition.items():\n",
    "        pos[node] = pos_communities[community]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _find_between_community_edges(g, partition):\n",
    "\n",
    "    edges = dict()\n",
    "\n",
    "    for (ni, nj) in g.edges():\n",
    "        ci = partition[ni]\n",
    "        cj = partition[nj]\n",
    "\n",
    "        if ci != cj:\n",
    "            try:\n",
    "                edges[(ci, cj)] += [(ni, nj)]\n",
    "            except KeyError:\n",
    "                edges[(ci, cj)] = [(ni, nj)]\n",
    "\n",
    "    return edges\n",
    "\n",
    "def _position_nodes(g, partition, **kwargs):\n",
    "    \"\"\"\n",
    "    Positions nodes within communities.\n",
    "    \"\"\"\n",
    "\n",
    "    communities = dict()\n",
    "    for node, community in partition.items():\n",
    "        try:\n",
    "            communities[community] += [node]\n",
    "        except KeyError:\n",
    "            communities[community] = [node]\n",
    "\n",
    "    pos = dict()\n",
    "    for ci, nodes in communities.items():\n",
    "        subgraph = g.subgraph(nodes)\n",
    "        pos_subgraph = nx.spring_layout(subgraph, **kwargs)\n",
    "        pos.update(pos_subgraph)\n",
    "\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install networkx 2.0 compatible version of python-louvain use:\n",
    "# pip install -U git+https://github.com/taynaud/python-louvain.git@networkx2\n",
    "from community import community_louvain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communities around the word \"problem\"\n",
    "# To save picture, right click on the picture and select \"Save image as...\"\n",
    "matplotlib.rcParams['figure.figsize'] = (40, 40)\n",
    "G=nx.ego_graph(G=graph_of_words, radius=1, n='problem')\n",
    "partition = community_louvain.best_partition(G)\n",
    "pos = community_layout(g=G, partition=partition)\n",
    "nx.draw(G, pos, node_color=list(partition.values()), \n",
    "        labels=dict((n,n) for n,d in G.nodes(data=True)), font_color='black', font_size=16, font_weight='bold',\n",
    "       edge_color='lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(G['problem'].items())).rename(columns={0:'word', 1:'weight_attr'}) \\\n",
    "    .assign(weight = lambda df: df.weight_attr.map(lambda cell: cell['weight'])) \\\n",
    "    .drop(['weight_attr'], axis=1) \\\n",
    "    .sort_values('weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Around the word \"issue\"\n",
    "G=nx.ego_graph(G=graph_of_words, radius=1, n='issue')\n",
    "partition = community_louvain.best_partition(G)\n",
    "pos = community_layout(g=G, partition=partition)\n",
    "matplotlib.rcParams['figure.figsize'] = (40, 40)\n",
    "nx.draw(G, pos, node_color=list(partition.values()), \n",
    "        labels=dict((n,n) for n,d in G.nodes(data=True)), font_color='black', font_size=16, font_weight='bold',\n",
    "       edge_color='lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Around the combination of each 2 words\n",
    "G=nx.compose_all([nx.ego_graph(G=graph_of_words, radius=1, n='issue'), \n",
    "                 nx.ego_graph(G=graph_of_words, radius=1, n='problem')])\n",
    "partition = community_louvain.best_partition(G)\n",
    "pos = community_layout(g=G, partition=partition)\n",
    "matplotlib.rcParams['figure.figsize'] = (40, 40)\n",
    "nx.draw(G, pos, node_color=list(partition.values()), \n",
    "        labels=dict((n,n) for n,d in G.nodes(data=True)), font_color='black', font_size=16, font_weight='bold',\n",
    "       edge_color='lightgray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
